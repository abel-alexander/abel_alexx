import importlib
import pkg_resources
from packaging import version

# Check the version of llama_index
llama_index_version = pkg_resources.get_distribution("llama_index").version

# Define the imports based on the version
if version.parse(llama_index_version) < version.parse("0.8.0"):
    from llama_index import (
        VectorStoreIndex,
        SimpleDirectoryReader,
        ServiceContext,
        PromptTemplate,
        VectorIndexRetriever,
        RetrieverQueryEngine,
        SimilarityPostprocessor,
        KeywordNodePostprocessor,
        get_response_synthesizer,
        SentenceSplitter,
        ChromaVectorStore,
        StorageContext,
    )
else:
    from llama_index.core import (
        VectorStoreIndex,
        SimpleDirectoryReader,
        ServiceContext,
        PromptTemplate,
    )
    from llama_index.core.retrievers import VectorIndexRetriever
    from llama_index.core.query_engine import RetrieverQueryEngine
    from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor
    from llama_index.core.response_synthesizers import get_response_synthesizer
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.vector_stores.chroma import ChromaVectorStore
    from llama_index.core.storage.storage_context import StorageContext

# Rest of your code

# Example function using the dynamically imported modules
def load_text_and_get_chunks(path_to_pdfs):
    documents = SimpleDirectoryReader(path_to_pdfs).load_data()
    return documents

def load_llm():
    hf_token = os.getenv("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")

    if torch.cuda.is_available():
        # Load the model with quantized features for GPU
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        device = "cuda"
    else:
        # Load the model without quantization for CPU
        quantization_config = BitsAndBytesConfig()
        device = "cpu"

    model_name = "EleutherAI/gpt-neo-2.7B"  # Example model name

    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)

    # Load the model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        use_auth_token=hf_token,
        quantization_config
