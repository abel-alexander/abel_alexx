import re
from collections import Counter
import fitz  # PyMuPDF
import nltk
from nltk.corpus import stopwords

# Download NLTK stopwords (run once)
nltk.download("stopwords")

def extract_hyperlinks(pdf_path):
    """
    Extract hyperlinks from the PDF and return a mapping of {page_number: [list of target pages]}.
    """
    doc = fitz.open(pdf_path)
    link_map = {}

    for page_no in range(len(doc)):  # Iterate through pages
        page = doc[page_no]
        links = page.get_links()

        for link in links:
            if "page" in link:  # Internal PDF link
                try:
                    target_page = int(link["page"])  # Ensure it's an integer
                    if target_page > 0:  # Ignore links to page 1
                        if target_page not in link_map:
                            link_map[target_page] = []
                        link_map[target_page].append(page_no + 1)  # Store 1-indexed page numbers
                except (ValueError, TypeError):
                    continue  # Ignore non-integer page references

    return link_map

def extract_text_from_page(pdf_path, page_number):
    """
    Extract text from a specific page using PyMuPDF.
    """
    doc = fitz.open(pdf_path)
    if 0 <= page_number < len(doc):
        return doc[page_number].get_text("text")
    return ""

def get_top_keywords(text, top_n=10):
    """
    Process text, remove stopwords, and return top N most frequent keywords.
    """
    words = re.findall(r"\b[a-z]+\b", text.lower())  # Extract words
    filtered_words = [word for word in words if word not in stopwords.words("english")]
    return Counter(filtered_words).most_common(top_n)

if __name__ == "__main__":
    pdf_path = "sample.pdf"  # Replace with your PDF file

    # Step 1: Extract hyperlinks and target pages
    link_map = extract_hyperlinks(pdf_path)

    # Step 2: Extract keywords for each linked page
    keywords_by_page = {}

    for target_page, source_pages in link_map.items():
        text = extract_text_from_page(pdf_path, target_page)
        if text.strip():  # Ensure page has text
            keywords_by_page[target_page] = get_top_keywords(text)

    # Step 3: Print results
    print("Top Keywords for Pages Linked in the PDF:")
    for target_page, keywords in keywords_by_page.items():
        print(f"\nPage {target_page + 1} (Linked from pages: {link_map[target_page]}):")
        for word, freq in keywords:
            print(f"  {word}: {freq}")
